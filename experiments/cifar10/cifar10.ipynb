{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9db57a-b813-49b6-ac18-20cd74bc1153",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, ConcatDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms.v2 as v2\n",
    "from copy import deepcopy\n",
    "import sys\n",
    "sys.path.append(\"/n/home11/sambt/phlab-neurips25\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "from models.litmodels import SimCLRModel\n",
    "from models.networks import CustomResNet, MLP\n",
    "from data.datasets import CIFAR10Dataset\n",
    "from data.cifar import CIFAR5MDataset\n",
    "import data.data_utils as dutils\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, top_k_accuracy_score\n",
    "from utils.plotting import make_corner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a720c78-f9e4-4fbd-aa98-805bdbe2232a",
   "metadata": {},
   "source": [
    "# evaluate pre-trained model; train classifier on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f33135-df7e-4742-a453-52c2da686f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar = CIFAR10Dataset(\"resnet50\",num_workers=2,batch_size=1024)\n",
    "cifar_train_dataset = cifar.train_dataset\n",
    "cifar_test_dataset = cifar.test_dataset\n",
    "\n",
    "cifar5m_full = CIFAR5MDataset(\"resnet50\",[0],[(None,50_000)],grayscale=True)\n",
    "\n",
    "model = SimCLRModel.load_from_checkpoint('/n/home11/sambt/phlab-neurips25/runs/cifar10_simCLR_ResNet50_T0.5/lightning_logs/uj88ngsb/checkpoints/epoch=14-step=735.ckpt')\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9509b322-6e89-4910-b839-0ab612993d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_embeds = []\n",
    "cifar_labels = []\n",
    "cifar_train_embeds = []\n",
    "cifar_train_labels = []\n",
    "cifar5m_embeds = []\n",
    "cifar5m_labels = []\n",
    "\n",
    "for ims,labs in tqdm(DataLoader(cifar_test_dataset,batch_size=1024,shuffle=False)):\n",
    "    with torch.no_grad():\n",
    "        cifar_embeds.append(model.encoder(ims.to(device)).cpu().numpy())\n",
    "        cifar_labels.append(labs.numpy()) \n",
    "        \n",
    "for ims,labs in tqdm(DataLoader(cifar_train_dataset,batch_size=1024,shuffle=False)):\n",
    "    with torch.no_grad():\n",
    "        cifar_train_embeds.append(model.encoder(ims.to(device)).cpu().numpy())\n",
    "        cifar_train_labels.append(labs.numpy()) \n",
    "        \n",
    "for ims,labs in tqdm(DataLoader(cifar5m_full,batch_size=1024,shuffle=False)):\n",
    "    with torch.no_grad():\n",
    "        cifar5m_embeds.append(model.encoder(ims.to(device)).cpu().numpy())\n",
    "        cifar5m_labels.append(labs.numpy())\n",
    "        \n",
    "cifar_embeds = np.concatenate(cifar_embeds)\n",
    "cifar_labels = np.concatenate(cifar_labels)\n",
    "cifar_train_embeds = np.concatenate(cifar_train_embeds)\n",
    "cifar_train_labels = np.concatenate(cifar_train_labels)\n",
    "cifar5m_embeds = np.concatenate(cifar5m_embeds)\n",
    "cifar5m_labels = np.concatenate(cifar5m_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1256f0-ecb2-46fe-ae73-786c0d71eae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_corner(cifar_embeds,cifar_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca33c86-0c57-46a5-8a2e-2fc730c3cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_label=0\n",
    "regular = cifar_embeds[cifar_labels==sel_label]\n",
    "shift = cifar5m_embeds[cifar5m_labels==sel_label]\n",
    "make_corner(np.concatenate([regular,shift],axis=0),\n",
    "            labels=np.concatenate([np.zeros(len(regular)),np.ones(len(shift))]),\n",
    "            label_names={0:\"cifar10\",1:\"cifar5m\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5563128e-fb86-4ae0-b656-1d4a5578d348",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models.networks import MLP\n",
    "classifier = MLP(4,[10,10],10,dropout=0.2,activation='tanh')\n",
    "classifier = classifier.to(device)\n",
    "optimizer = torch.optim.AdamW(classifier.parameters(),lr=1e-3)\n",
    "\n",
    "loader = DataLoader(TensorDataset(torch.tensor(cifar_train_embeds),torch.tensor(cifar_train_labels)),batch_size=512,shuffle=True,num_workers=2)\n",
    "val_loader = DataLoader(TensorDataset(torch.tensor(cifar_embeds),torch.tensor(cifar_labels)),batch_size=512,shuffle=True,num_workers=2)\n",
    "\n",
    "n_epoch = 20\n",
    "best_state = None\n",
    "best_loss = 9999\n",
    "pbar = tqdm(range(n_epoch))\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for i in pbar:\n",
    "    losses = []\n",
    "    for x,y in loader:\n",
    "        #with torch.no_grad():\n",
    "        #    embed = model.encoder(x.to(device)).detach()\n",
    "        out = classifier(x.to(device))\n",
    "        loss = F.cross_entropy(out,y.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    losses = np.mean(losses)\n",
    "    train_losses.append(losses)\n",
    "    \n",
    "    losses = []\n",
    "    for x,y in val_loader:\n",
    "        with torch.no_grad():\n",
    "        #    embed = model.encoder(x.to(device))\n",
    "            out = classifier(x.to(device))\n",
    "            loss = F.cross_entropy(out,y.to(device))\n",
    "        losses.append(loss.item())\n",
    "    losses = np.mean(losses)\n",
    "    val_losses.append(losses)\n",
    "    if losses < best_loss:\n",
    "        best_loss = losses\n",
    "        best_state = classifier.state_dict()\n",
    "        \n",
    "    pbar.set_postfix_str(f\"train:{train_losses[-1]:.5f}, val:{val_losses[-1]:.5f}\")\n",
    "\n",
    "classifier.load_state_dict(best_state)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "x = np.arange(1,n_epoch+1)\n",
    "plt.plot(x,train_losses,label='train')\n",
    "plt.plot(x,val_losses,label='val')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd2b89d-505a-4523-9389-4c4404920ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_cifar = []\n",
    "probs_cifar = []\n",
    "preds_cifar5m = []\n",
    "probs_cifar5m = []\n",
    "\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    for x in tqdm(torch.split(torch.tensor(cifar_embeds),4096)):\n",
    "        preds_cifar.append(classifier(x.to(device)).cpu().numpy())\n",
    "        probs_cifar.append(F.softmax(torch.tensor(preds_cifar[-1]),dim=1).numpy())\n",
    "    for x in tqdm(torch.split(torch.tensor(cifar5m_embeds),4096)):\n",
    "        preds_cifar5m.append(classifier(x.to(device)).cpu().numpy())\n",
    "        probs_cifar5m.append(F.softmax(torch.tensor(preds_cifar5m[-1]),dim=1).numpy())\n",
    "preds_cifar = np.concatenate(preds_cifar)\n",
    "probs_cifar = np.concatenate(probs_cifar)\n",
    "preds_cifar5m = np.concatenate(preds_cifar5m)\n",
    "probs_cifar5m = np.concatenate(probs_cifar5m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de53c84d-628c-4fdd-9c8b-055d4a56b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Embedding space classifier metrics for CIFAR10 test set\")\n",
    "auc = roc_auc_score(cifar_labels,probs_cifar,multi_class='ovr')\n",
    "print(\"OVR auc = \",auc)\n",
    "auc = roc_auc_score(cifar_labels,probs_cifar,multi_class='ovo')\n",
    "print(\"OVO auc = \",auc)\n",
    "for k in range(1,6):\n",
    "    topk = top_k_accuracy_score(cifar_labels,probs_cifar,k=k)\n",
    "    print(f\"Top {k} acc = \",topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22500f9-e2a1-4da3-8464-27ce0826c55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Embedding space classifier metrics for CIFAR5m tiny set\")\n",
    "auc = roc_auc_score(cifar5m_labels,probs_cifar5m,multi_class='ovr')\n",
    "print(\"OVR auc = \",auc)\n",
    "auc = roc_auc_score(cifar5m_labels,probs_cifar5m,multi_class='ovo')\n",
    "print(\"OVO auc = \",auc)\n",
    "for k in range(1,6):\n",
    "    topk = top_k_accuracy_score(cifar5m_labels,probs_cifar5m,k=k)\n",
    "    print(f\"Top {k} acc = \",topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e003cc3-2b14-4ae6-a63a-402823c50997",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_probs_cifar5m = np.max(probs_cifar5m,axis=1)\n",
    "plt.figure(figsize=(8,6))\n",
    "h,bins,_ = plt.hist(top_probs_cifar5m,bins=np.linspace(0,1,100),histtype='step',density=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25215142-2519-48ba-9dd7-55e3f6b4730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((bins[1:]+bins[:-1])/2,np.cumsum(h)/h.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5ec166-b0f3-4888-a38b-2ebe1ff46141",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar5m_finetune_dset = cifar5m_full.subselection(top_probs_cifar5m > 0.5)\n",
    "cifar5m_finetune_train, cifar5m_finetune_test = cifar5m_finetune_dset.random_split(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d35a6c-9abe-422e-93b3-2cb1894d69b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "h=plt.hist(cifar5m_finetune_train.dataset[1],bins=np.arange(-0.5,10.5),histtype='step',density=True)\n",
    "h=plt.hist(cifar5m_finetune_test.dataset[1],bins=np.arange(-0.5,10.5),histtype='step',density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a558f7-56f4-4a58-bddc-1259ad84b3f5",
   "metadata": {},
   "source": [
    "# fine tune v1: backbone only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0919fc78-1d5c-4201-bff8-a6171941f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.finetune import FineTuner\n",
    "from models.revgrad import GradientReversal\n",
    "\n",
    "weights = torch.load('/n/home11/sambt/phlab-neurips25/runs/cifar10_simCLR_ResNet50_T0.5/lightning_logs/uj88ngsb/checkpoints/epoch=14-step=735.ckpt',\n",
    "                    map_location='cpu')\n",
    "sd = weights['state_dict']\n",
    "encoder_weights = {k.replace(\"encoder.\",\"\"):v for k,v in sd.items() if \"encoder\" in k}\n",
    "projector_weights = {k.replace(\"projector.\",\"\"):v for k,v in sd.items() if \"projector\" in k}\n",
    "\n",
    "encoder = CustomResNet(\"resnet50\",[512,256,128],4)\n",
    "encoder.load_state_dict(encoder_weights)\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "projector = MLP(4,[4],4)\n",
    "projector.load_state_dict(projector_weights)\n",
    "\n",
    "corrector = MLP(4,[16,4],1,activation='relu')\n",
    "corrector = nn.Sequential(GradientReversal(alpha=1.0),corrector)\n",
    "#corrector = None\n",
    "\n",
    "tuner = FineTuner(encoder,projector,corrector).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49afaf81-19c1-44c0-8327-6fdc5a723cc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuner = tuner.eval()\n",
    "\n",
    "cifar_embeds_pretune = []\n",
    "cifar_labels_pretune = []\n",
    "for batch in tqdm(DataLoader(cifar_test_dataset,batch_size=1024,shuffle=False,num_workers=2)):\n",
    "    x,labels = batch\n",
    "    with torch.no_grad():\n",
    "        cifar_embeds_pretune.append(tuner.encoder(x.to(device)).cpu().numpy())\n",
    "        cifar_labels_pretune.append(labels.numpy())\n",
    "cifar_embeds_pretune = np.concatenate(cifar_embeds_pretune)\n",
    "cifar_labels_pretune = np.concatenate(cifar_labels_pretune)\n",
    "\n",
    "\n",
    "#cifar5m_indpt = CIFAR5MDataset(\"resnet50\",[1],[(None,10_000)],grayscale=True)\n",
    "cifar5m_test_embed_pretune = []\n",
    "cifar5m_test_labels_pretune = []\n",
    "for batch in tqdm(DataLoader(cifar5m_finetune_test,batch_size=1024,shuffle=False,num_workers=2)):\n",
    "    x,labels = batch\n",
    "    with torch.no_grad():\n",
    "        cifar5m_test_embed_pretune.append(tuner.encoder(x.to(device)).cpu().numpy())\n",
    "        cifar5m_test_labels_pretune.append(labels.numpy())\n",
    "cifar5m_test_embed_pretune = np.concatenate(cifar5m_test_embed_pretune)\n",
    "cifar5m_test_labels_pretune = np.concatenate(cifar5m_test_labels_pretune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7a2ae6-92c9-4992-9bff-a2d5d7ce3cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tuner\n",
    "del x, dset_label, batch, labels, domain_labels, preds, pos_mask, optimizer, loss\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27af4f8-d70d-4b6f-8c39-05542a0e237d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models.losses import SupervisedSimCLRLoss\n",
    "\n",
    "num_epoch = 10\n",
    "patience_thresh = 100\n",
    "criterion = SupervisedSimCLRLoss(temperature=0.5)\n",
    "optimizer = torch.optim.AdamW(tuner.parameters(),lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=num_epoch,eta_min=1e-5)\n",
    "\n",
    "best_state = None\n",
    "best_loss = 9999\n",
    "patience = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_simclr_losses = []\n",
    "train_class_losses = []\n",
    "val_simclr_losses = []\n",
    "val_class_losses = []\n",
    "\n",
    "train_loader = DataLoader(dutils.ConcatWithLabels([cifar_train_dataset,cifar5m_finetune_train],[0,1]),\n",
    "                          batch_size=512,shuffle=True,num_workers=2,drop_last=True)\n",
    "val_loader = DataLoader(dutils.ConcatWithLabels([cifar_test_dataset,cifar5m_finetune_test],[0,1]),\n",
    "                        batch_size=512,shuffle=True,num_workers=2,drop_last=True)\n",
    "\n",
    "\n",
    "tuner = tuner.train()\n",
    "#pbar = tqdm(range(num_epoch),position=0,leave=True)\n",
    "lambda_class = 1.0\n",
    "for i in range(num_epoch):\n",
    "    losses = []\n",
    "    losses_simclr = []\n",
    "    losses_class = []\n",
    "    for batch in tqdm(train_loader):\n",
    "        x,dset_label = batch\n",
    "        x,labels = x\n",
    "        h = tuner.encoder(x.to(device))\n",
    "        preds = tuner.corrector(h)\n",
    "        domain_labels = (dset_label==1).float().to(device).unsqueeze(1)\n",
    "        pos_mask = (labels.unsqueeze(1) == labels.unsqueeze(1).T).to(device) & (domain_labels == domain_labels.T)\n",
    "        z = tuner.projector(h)\n",
    "        z = F.normalize(z,dim=1).unsqueeze(1) # normalize the projection for simclr loss\n",
    "        #loss_simclr = criterion(z, labels=labels)\n",
    "        loss_simclr = criterion(z, mask=pos_mask)\n",
    "        loss_class = F.binary_cross_entropy_with_logits(preds,domain_labels)\n",
    "        loss = loss_simclr + lambda_class*loss_class\n",
    "        #loss = loss_simclr\n",
    "        #loss = loss_class\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        losses_simclr.append(loss_simclr.item())\n",
    "        losses_class.append(loss_class.item())\n",
    "    train_losses.append(np.mean(losses))\n",
    "    train_simclr_losses.append(np.mean(losses_simclr))\n",
    "    train_class_losses.append(np.mean(losses_class))\n",
    "    \n",
    "    losses = []\n",
    "    losses_simclr = []\n",
    "    losses_class = []\n",
    "    aucs = []\n",
    "    for batch in tqdm(val_loader):\n",
    "        x,dset_label = batch\n",
    "        x,labels = x\n",
    "        with torch.no_grad():\n",
    "            h = tuner.encoder(x.to(device))\n",
    "            preds = tuner.corrector(h)\n",
    "            domain_labels = (dset_label==1).float().to(device).unsqueeze(1)\n",
    "            pos_mask = (labels.unsqueeze(1) == labels.unsqueeze(1).T).to(device) & (domain_labels == domain_labels.T)\n",
    "            z = tuner.projector(h)\n",
    "            z = F.normalize(z,dim=1).unsqueeze(1) # normalize the projection for simclr loss\n",
    "            #loss_simclr = criterion(z, labels=labels)\n",
    "            loss_simclr = criterion(z, mask=pos_mask)\n",
    "            loss_class = F.binary_cross_entropy_with_logits(preds,domain_labels)\n",
    "            loss = loss_simclr + lambda_class*loss_class\n",
    "            #loss = loss_simclr\n",
    "            #loss = loss_class\n",
    "            losses.append(loss.item())\n",
    "            losses_simclr.append(loss_simclr.item())\n",
    "            losses_class.append(loss_class.item())\n",
    "            aucs.append(roc_auc_score(domain_labels.cpu().numpy()[:,0],preds.cpu().numpy()[:,0]))\n",
    "    losses = np.mean(losses)\n",
    "    val_losses.append(losses)\n",
    "    val_simclr_losses.append(np.mean(losses_simclr))\n",
    "    val_class_losses.append(np.mean(losses_class))\n",
    "    if losses < best_loss:\n",
    "        best_loss = losses\n",
    "        best_state = tuner.state_dict()\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience == patience_thresh:\n",
    "            print(f\"{patience_thresh} epochs of no improvement, stopping\")\n",
    "            break\n",
    "        \n",
    "    scheduler.step()\n",
    "    \n",
    "    #pbar.set_postfix_str(f\"Train: {train_losses[-1]:.5f}, Val:{val_losses[-1]:.5f}\")\n",
    "    print(f\"Epoch {i+1}, Train: {train_losses[-1]:.5f}, Val:{val_losses[-1]:.5f}, Val auc: {np.mean(aucs):.5f}\")\n",
    "    print(f\"\\t Train (simclr): {train_simclr_losses[-1]:.5f}, Val (simclr):{val_simclr_losses[-1]:.5f}\")\n",
    "    print(f\"\\t Train (class): {train_class_losses[-1]:.5f}, Val (class):{val_class_losses[-1]:.5f}\")\n",
    "    \n",
    "tuner.load_state_dict(best_state)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "x = np.arange(1,len(train_losses)+1)\n",
    "plt.plot(x,train_losses,label='train',color=\"C0\")\n",
    "plt.plot(x,val_losses,label='val',color=\"C1\")\n",
    "plt.plot(x,train_simclr_losses,label='train (simclr)',color=\"C0\",linestyle='--')\n",
    "plt.plot(x,val_simclr_losses,label='val (simclr)',color=\"C1\",linestyle='--')\n",
    "plt.plot(x,train_class_losses,label='train (class)',color=\"C0\",linestyle=':')\n",
    "plt.plot(x,val_class_losses,label='val (class)',color=\"C1\",linestyle=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a215d50-2cc0-4fe2-8a0e-38326f016e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.load_state_dict(best_state)\n",
    "tuner=tuner.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e976c6ac-0c02-4b90-8a11-09f0d8fcd908",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = tuner.eval()\n",
    "\n",
    "cifar_embeds_tune = []\n",
    "cifar_labels_tune = []\n",
    "cifar_domainProbs_tune = []\n",
    "for batch in tqdm(DataLoader(cifar_test_dataset,batch_size=1024,shuffle=True,num_workers=2)):\n",
    "    x,labels = batch\n",
    "    with torch.no_grad():\n",
    "        cifar_embeds_tune.append(tuner.encoder(x.to(device)).cpu().numpy())\n",
    "        cifar_labels_tune.append(labels.numpy())\n",
    "        cifar_domainProbs_tune.append(torch.sigmoid(tuner.corrector(torch.tensor(cifar_embeds_tune[-1]).to(device))).cpu().numpy())\n",
    "cifar_embeds_tune = np.concatenate(cifar_embeds_tune)\n",
    "cifar_labels_tune = np.concatenate(cifar_labels_tune)\n",
    "cifar_domainProbs_tune = np.concatenate(cifar_domainProbs_tune)\n",
    "\n",
    "\n",
    "#cifar5m_indpt = CIFAR5MDataset(\"resnet50\",[1],[(None,10_000)],grayscale=True)\n",
    "cifar5m_test_embed_tune = []\n",
    "cifar5m_test_labels_tune = []\n",
    "cifar5m_test_domainProbs_tune = []\n",
    "for batch in tqdm(DataLoader(cifar5m_finetune_train,batch_size=1024,shuffle=True,num_workers=2)):\n",
    "    x,labels = batch\n",
    "    with torch.no_grad():\n",
    "        cifar5m_test_embed_tune.append(tuner.encoder(x.to(device)).cpu().numpy())\n",
    "        cifar5m_test_labels_tune.append(labels.numpy())\n",
    "        cifar5m_test_domainProbs_tune.append(torch.sigmoid(tuner.corrector(torch.tensor(cifar5m_test_embed_tune[-1]).to(device))).cpu().numpy())\n",
    "cifar5m_test_embed_tune = np.concatenate(cifar5m_test_embed_tune)\n",
    "cifar5m_test_labels_tune = np.concatenate(cifar5m_test_labels_tune)\n",
    "cifar5m_test_domainProbs_tune = np.concatenate(cifar5m_test_domainProbs_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468917f4-ce5a-47dd-a40c-f82bb3fb18c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "bins = np.linspace(0,1,100)\n",
    "h = plt.hist(cifar_domainProbs_tune,bins=bins,density=True,histtype='step')\n",
    "h = plt.hist(cifar5m_test_domainProbs_tune,bins=bins,density=True,histtype='step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbf4f83-d765-4262-8219-7ea59a91fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plotting import make_corner\n",
    "sel_label=2\n",
    "regular = cifar_embeds_tune[cifar_labels_tune==sel_label]\n",
    "shift = cifar5m_test_embed_tune[cifar5m_test_labels_tune==sel_label]\n",
    "make_corner(np.concatenate([regular,shift],axis=0),\n",
    "            labels=np.concatenate([np.zeros(len(regular)),np.ones(len(shift))]),\n",
    "            label_names={0:\"cifar10\",1:\"cifar5m\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c262ad99-6822-4a39-bc9d-fc5f05fecfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plotting import make_corner\n",
    "sel_label=2\n",
    "regular = cifar_embeds_tune[cifar_labels_tune==sel_label]\n",
    "shift = cifar_embeds_pretune[cifar_labels_pretune==sel_label]\n",
    "make_corner(np.concatenate([regular,shift],axis=0),\n",
    "            labels=np.concatenate([np.zeros(len(regular)),np.ones(len(shift))]),\n",
    "            label_names={0:\"cifar10\",1:\"pretune\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc872a3-a9e5-4f50-9f82-eb5552693b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar5m_indpt = CIFAR5MDataset(\"resnet50\",[1],[(None,100_000)],grayscale=True)\n",
    "\n",
    "cifar_embeds_tuned = []\n",
    "cifar_labels_tuned = []\n",
    "cifar_train_embeds_tuned = []\n",
    "cifar_train_labels_tuned = []\n",
    "cifar5m_embeds_tuned = []\n",
    "cifar5m_labels_tuned = []\n",
    "\n",
    "for ims,labs in tqdm(DataLoader(cifar_test_dataset,batch_size=1024,shuffle=False)):\n",
    "    with torch.no_grad():\n",
    "        cifar_embeds_tuned.append(tuner.encoder(ims.to(device)).cpu().numpy())\n",
    "        cifar_labels_tuned.append(labs.numpy()) \n",
    "        \n",
    "for ims,labs in tqdm(DataLoader(cifar_train_dataset,batch_size=1024,shuffle=False)):\n",
    "    with torch.no_grad():\n",
    "        cifar_train_embeds_tuned.append(tuner.encoder(ims.to(device)).cpu().numpy())\n",
    "        cifar_train_labels_tuned.append(labs.numpy()) \n",
    "        \n",
    "for ims,labs in tqdm(DataLoader(cifar5m_indpt,batch_size=1024,shuffle=False)):\n",
    "    with torch.no_grad():\n",
    "        cifar5m_embeds_tuned.append(tuner.encoder(ims.to(device)).cpu().numpy())\n",
    "        cifar5m_labels_tuned.append(labs.numpy())\n",
    "        \n",
    "cifar_embeds_tuned = np.concatenate(cifar_embeds_tuned)\n",
    "cifar_labels_tuned = np.concatenate(cifar_labels_tuned)\n",
    "cifar_train_embeds_tuned = np.concatenate(cifar_train_embeds_tuned)\n",
    "cifar_train_labels_tuned = np.concatenate(cifar_train_labels_tuned)\n",
    "cifar5m_embeds_tuned = np.concatenate(cifar5m_embeds_tuned)\n",
    "cifar5m_labels_tuned = np.concatenate(cifar5m_labels_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10963a0d-c914-4be3-86ae-453d2a27bbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plotting import make_corner\n",
    "sel_label=0\n",
    "make_corner(np.concatenate([cifar_embeds_tuned[cifar_labels_tuned==sel_label],\n",
    "                            cifar5m_embeds_tuned[cifar5m_labels_tuned==sel_label]],axis=0),\n",
    "            labels=np.concatenate([np.zeros(len(cifar_embeds_tuned[cifar_labels_tuned==sel_label])),\n",
    "                                   np.ones(len(cifar5m_embeds_tuned[cifar5m_labels_tuned==sel_label]))]),\n",
    "            label_names={0:\"cifar10\",1:\"cifar5m\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c14d0e-9758-4bc0-9af4-971edfd89801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3018d539-a233-41ae-a8f0-391d79036cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.networks import MLP\n",
    "classifier = MLP(4,[10,10],10,dropout=0.2,activation='tanh')\n",
    "classifier = classifier.to(device)\n",
    "optimizer = torch.optim.AdamW(classifier.parameters(),lr=1e-3)\n",
    "\n",
    "cifar_dset.batch_size = 512\n",
    "loader = DataLoader(TensorDataset(torch.tensor(cifar_train_embeds),torch.tensor(cifar_train_labels)),batch_size=512,shuffle=True,num_workers=2)\n",
    "val_loader = DataLoader(TensorDataset(torch.tensor(test_cifar_embeds),torch.tensor(cifar_labels)),batch_size=512,shuffle=True,num_workers=2)\n",
    "\n",
    "n_epoch = 20\n",
    "best_state = None\n",
    "best_loss = 9999\n",
    "pbar = tqdm(range(n_epoch))\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for i in pbar:\n",
    "    losses = []\n",
    "    for x,y in loader:\n",
    "        #with torch.no_grad():\n",
    "        #    embed = model.encoder(x.to(device)).detach()\n",
    "        out = classifier(x.to(device))\n",
    "        loss = F.cross_entropy(out,y.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    losses = np.mean(losses)\n",
    "    train_losses.append(losses)\n",
    "    \n",
    "    losses = []\n",
    "    for x,y in val_loader:\n",
    "        with torch.no_grad():\n",
    "        #    embed = model.encoder(x.to(device))\n",
    "            out = classifier(x.to(device))\n",
    "            loss = F.cross_entropy(out,y.to(device))\n",
    "        losses.append(loss.item())\n",
    "    losses = np.mean(losses)\n",
    "    val_losses.append(losses)\n",
    "    if losses < best_loss:\n",
    "        best_loss = losses\n",
    "        best_state = classifier.state_dict()\n",
    "        \n",
    "    pbar.set_postfix_str(f\"train:{train_losses[-1]:.5f}, val:{val_losses[-1]:.5f}\")\n",
    "\n",
    "classifier.load_state_dict(best_state)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "x = np.arange(1,n_epoch+1)\n",
    "plt.plot(x,train_losses,label='train')\n",
    "plt.plot(x,val_losses,label='val')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889b7ad4-4cab-4f6a-af3d-0a32f292befe",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(TensorDataset(torch.tensor(cifar_test_embed),torch.tensor(cifar_test_labels)),batch_size=512,shuffle=False,num_workers=2)\n",
    "val_loader_5m = DataLoader(TensorDataset(torch.tensor(cifar5m_test_embed),torch.tensor(cifar5m_test_labels)),batch_size=512,shuffle=False,num_workers=2)\n",
    "\n",
    "preds_cifar = []\n",
    "preds_cifar5m = []\n",
    "\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    for x,y in val_loader:\n",
    "        preds_cifar.append(classifier(x.to(device)).cpu().numpy())\n",
    "    for x,y in val_loader_5m:\n",
    "        preds_cifar5m.append(classifier(x.to(device)).cpu().numpy())\n",
    "preds_cifar = np.concatenate(preds_cifar)\n",
    "preds_cifar5m = np.concatenate(preds_cifar5m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7851e2-98d2-4b19-a818-d03fc8c211c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Embedding space classifier metrics for CIFAR10 test set\")\n",
    "auc = roc_auc_score(F.one_hot(torch.tensor(cifar_test_labels)).numpy(),preds_cifar)\n",
    "print(\"auc = \",auc)\n",
    "for k in range(1,6):\n",
    "    topk = top_k_accuracy_score(cifar_test_labels,preds_cifar,k=k)\n",
    "    print(f\"Top {k} acc = \",topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8989e0-adc7-40f0-8caa-4ca7cdc146f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Embedding space classifier metrics for CIFAR5m tiny set\")\n",
    "auc = roc_auc_score(F.one_hot(torch.tensor(cifar5m_test_labels)).numpy(),preds_cifar5m)\n",
    "print(\"auc = \",auc)\n",
    "for k in range(1,6):\n",
    "    topk = top_k_accuracy_score(cifar5m_test_labels,preds_cifar5m,k=k)\n",
    "    print(f\"Top {k} acc = \",topk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mamba-torch_gpu]",
   "language": "python",
   "name": "conda-env-mamba-torch_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
